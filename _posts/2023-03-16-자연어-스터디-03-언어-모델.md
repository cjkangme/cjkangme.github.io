

---
title: [자연어 스터디] 03. 언어 모델
date: 2023-03-16 11:24:49.511 +0000
categories: [딥러닝]
tags: ['스터디', '자연어처리']
description: 공모전 대비 자연어 스터디 2회차 (1)

math: true
---

> [딥러닝을 이용한 자연어 처리 입문 교재](https://wikidocs.net/21695)를 바탕으로 작성되었습니다.

> 노션에 정리된 것을 거의 그대로 옮겨왔습니다.일부 텍스트 깨짐 등이 있을 수 있습니다.

# 03-01. 언어 모델(Language Model)이란?

- 언어 모델은 단어 시퀀스(문장)에 확률을 할당하는 모델이다.
- **통계를 이용한 방법, 신경망을 이용한 방법으로 구분**할 수 있다. 최근에는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여준다.

## 1. 언어 모델

- 확률 기반으로 가장 자연스러운 단어 시퀀스를 찾아내는 모델이다. 가장 보편적인 것은 **이전 단어들이 주어졌을 때 다음 단어를 예측**하는 것이다.
- 이후 배우게 될 BERT 모델과 같이 양쪽의 문맥을 통해 가운데의 빈 단어를 맞추는 유형도 있다.
- 언어 모델에 -ing가 붙은 **언어 모델링(Language Modeling)은 주어진 단어로부터 아직 모르는 단어를 예측하는 작업**을 말한다.

## 2. 단어 시퀀스의 확률 할당

- 확률을 통해 다음 시퀀스를 예측하는 원리로 다음과 같은 문제를 풀 수 있다.

### 1) 기계 번역

```python
P(나는 버스를 탔다) > P(나는 버스를 태운다)
```

- 좌측의 문장 확률이 더 높다.

### 2) 오타 교정

```python
아버지가 방에 
P(들어가신다) > P(들고가신다)
```

- 좌측의 문장 확률이 더 높다.

## 3. 주어진 이전 단어로 다음 단어 예측하기

- 조건부 확률로 표현할 수 있다.

### 1) 단어 시퀀스의 확률

- n개의 단어 w로 구성된 단어 시퀀스 **W**의 확률은 다음과 같다.
    
    $$ \\P(W) = P(w_1, w_2, w_3, w_4, w_5, ... ,w_n) $$
    

### 2) 다음 단어 등장 확률

- 단어를 순차적으로 하나씩 추가해가면서 다음 단어의 등장 확률을 알 수 있다.
- 예를 들어 **W** 4번째 단어의 등장 확률은 다음과 같이 조건부 확률로 나타낼 수 있다.
    $$ \\P(w_4 | w_1, w_2, w_3) $$
    
    - `|` 기호는 조건부 확률을 의미한다.
- 즉 **W**의 확률은 모든 단어가 예측되고 나서야 알 수 있다.

## 4. 검색 엔진에서의 언어 모델

- 구글, 네이버의 자동 완성 기능이 이러한 다음 단어를 예측하는 언어 모델을 사용한다.

# 03-02 통계적 언어 모델(Statistical Languate Model; SLM)

- 통계학적 접근 방법의 언어 모델을 말한다.

## 1. 조건부 확률

- A가 일어났을 때 B가 일어날 조건부 확률 : $$ P(B|A) $$
    - $$ P(B|A) = P(A \cap B)/P(A) $$
    - $$ P(A \cap B) = P(B|A) * P(A) $$
- 사건이 더 많아지면 아래와 같이 표현할 수 있다.
    - $$ P(A \cap B \cap C \cap D) = P(A)P(B|A)P(C|A \cap B)P(D|A \cap B \cap C) $$
    - 이렇게 조건부 확률이 연결되는 법칙을 확률의 연쇄 법칙이라고 한다.

## 2. 문장에 대한 확률

$$  $$
P(W) = P(w_1, w_2, w_3, w_4, w_5, ... ,w_n) \\ = P(w_1)P(w_1|w_2)P(w_3|w_1 \cap w_2)P(w_4|w_1 \cap w_2 \cap w_3) ...
$$  $$

## 3. 카운트 기반 접근

- SLM은 전체 코퍼스 데이터에서 추출한 전체 단어와 각 단어의 카운트가 담긴 vocab을 만든다. 이 카운트를 바탕으로 확률을 계산한다.
- I like to drink 가 총 100번 나왔을 때, hot이 30번 나왔다고 하면, `P(hot|I like to drink) = 30%` 이다.

## 4. 카운트 기반 접근의 한계

- 카운트 기반 SLM이 보다 높은 성능을 내기 위해서는 기계가 훈련하기 위해 정말 방대한 데이터가 필요하다.
- “I like to drink hot chocolate”이라는 문장을 만들고 싶은데, 학습한 코퍼스에서 “I like to drink”가  등장한 사례가 없으면 `P(hot|I like to drink)` 에서 분모가 0이되어 아예 확률을 도출할 수 없다.
    - 이러한 확률을 정의되지 않는 확률이라고 하고, 이러한 문제를 **희소 문제(sparsity problem)**이라고 한다.
- 희소 문제를 해결하기 위한 다양한 기법이 제시 되었으나 결국 근본적인 해결책은 되지 못했다.

# 03-03 N-gram 언어 모델(N-gram Language Model)

- n-gram 언어 모델은 SLM의 일종으로 이전에 모든 단어를 고려하는 대신, 이전의 n개의 단어만 고려하는 접근 방법을 사용한다.

## 1. 코퍼스에서 카운트하지 못하는 경우의 감소

- 전체 코퍼스 데이터에서 `I like to drink` 라는 문장보다 `to drink` 라는 문장이 존재할 가능성이 더 높다. 즉 카운트 할 수 있는 가능성을 더 높일 수 있다.
- 이렇게 하면 희소 문제에서 정의되지 않는 확률이 존재할 가능성을 낮출 수 있다.

## 2. N-gram

- n-gram에서 n은 이전에 n개까지의 단어 나열을 센다는 의미이다.
    - unigrams(n=1) : I, like, to, drink
    - bigrams(n=2) : I like, like to, to drink
    - trigrams(n=3) : I like to, like to drink
    - 4-grams(n=4) : I like to drink
- `I like to drink` 다음에 올 문장을 예측할 때, 앞의 n-1개의 단어만을 고려하여 n개의 연속적인 단어 나열을 찾는다.
    - n=3 인 경우 $$ P(\text{hot}|\text{to drink}) = \frac{\text{count(to drink hot})}{\text{count(to drink)}} $$

## 3. N-gram 언어 모델의 한계

- n-gram은 n개 이전의 단어에 대해서는 완전히 잊어버린다는 문제가 있다.
    - 예를 들어 `I hate to drink` 라는 문장을 완성하려 할 때, trigrams라면 `I like to drink` 다음에 올 문장을 찾는 것과 차이가 없어지게 된다.
- 즉 희소 문제를 어느정도 해결할 수 있지만, 정확도는 전체 문장을 확인할 때보다 떨어진다.

### 1) 희소 문제

- 전체 문장을 확인하는 것보다는 희소 문제가 발생할 가능성이 낮지만, 여전히 희소 문제가 존재한다.

### 2) n의 trade-off 관계

- n이 커질수록 희소문제는 심각해지고, 모델 사이즈가 증가한다.
- 반면 n이 작아질수록 모델의 정확도가 감소한다.

## 4. 적용 분야(Domain)에 맞는 코퍼스 수집

- 특정 어플리케이션에 사용하기 위한 언어 모델이라면, 당연히 해당 도메인의 코퍼스를 사용해야 정확도가 증가할 것이다.

## 5. 인공 신경망 기반 언어 모델

- N-gram 언어 모델 역시 SLM의 본질적인 문제를 해결하지 못하였다, 이를 위한 대안으로 SLM보다 대체적으로 성능이 우수한 **인공 신경망 기반 언어 모델**을 많이 사용하고 있다.

# 03-04 한국어에서의 언어 모델

- 한국어는 특히 다음 단어를 예측하기가 까다롭다.

## 1. 한국어는 어순이 중요하지 않다.

- 한국어는 어순이 뒤바뀌어도 대략 의미가 통하기 때문에, 다음 단어로 어떤 단어든 등장할 수 있다.
    - 어제 친구랑 밥을 먹으러 홍대에 갔다.
    - 친구랑 어제 밥을 먹으러 홍대에 갔다.
    - 어제 홍대에 친구랑 밥을 먹으러 갔다.
    - 어제 홍대에 갔다. 친구랑 밥을 먹으러.
    - 어제 친구랑 밥을 먹으러 갔다. 홍대에
- 이렇게 단어 순서가 뒤죽박죽으로 바뀔 수 있기 때문에 확률 기반의 언어 모델이 다음 단어를 예측하기 어렵다.

## 2. 한국어는 교착어이다.

- 한국어는 교착어이기 때문에 조사 등이 존재한다. 때문에 띄어쓰기인 어절 단위로 토큰화하면 발생 가능한 단어의 수가 굉장히 늘어난다.
    - 친구가, 친구는, 친구랑, 친구를, 친구와, 친구의
- 때문에 한국어에서 접사와 조사를 분리하는 것은 중요한 작업이되기도 한다.

## 3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다.

- 한국어의 띄어쓰기 문법은 현대에 정립되었으며, 심지어 외국인이 한국어 공부 책을 만들 때 영어와 대응하는 과정에서 시작되었다. 즉 한국어에서 띄어쓰기는 굉장히 생소한 개념이며,  띄어쓰기를 제대로 지킬 줄 아는 사람이 드물다.
- 이러다 보니 한국어 코퍼스는 띄어쓰기가 지켜지지 않는 경우가 많다. 이로 인해 토큰화가 잘 이루어지지 않으면 모델도 제대로 동작하지 않을 것이다.

# 03-05 펄플렉서티(Perplexity, PPL)

- PPL은 언어 모델의 성능을 수치화한 것이다.

## 1. 언어 모델의 평가 방법 : PPL

- Perplexed는 ‘헷갈리는’과 유사한 의미를 가진다. 즉 PPL은 헷갈리는 정도로 이해할 수 있다.
- PPL이 작을 수록 덜 헷갈리는, 즉 좋은 성능의 모델임을 의미한다.
- PPL은 문장의 길이로 정규화된 문장 확률의 역수이다.

$$  $$
PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\frac{1}{N}}=\sqrt[N]{\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}
$$  $$

- 예를들어 bigrams 언어 모델에서 `I like to drink`의 문장 확률이 0.5라면, PPL(to drink) = sqrt(2) ≒ 1.414가 된다.

## 2. 분기 계수 (Branching factor)

- PPL은 특정 시점에서 평균 적으로 몇 개의 선택지를 선택할 수 있는지를 의미한다.
- 가령 어떤 테스트 데이터에 대한 모델의 PPL이 10이었다면, 해당 모델은 평균적으로 다음 단어를 예측하는 모든 시점에서 평균 10개의 단어를 두고 고민했다고 볼 수 있다.
- 만약 테스트 데이터가 같다면, PPL이 낮을 수록 **해당 테스트 데이터에 대한 성능**이 더 좋다고 할 수 있다. **(모든 데이터에 해당하는 것이 아님을 유의)**
- 만약 두 모델의 성능을 PPL을 통해 비교하고자 할 때는 동일한 테스트 데이터를 사용해야 신뢰도가 높다.

        