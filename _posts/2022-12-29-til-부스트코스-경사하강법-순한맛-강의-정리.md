

        ---
        title: [TIL] 부스트코스 경사하강법 - 순한맛 강의 정리
        date: 2022-12-29 13:40:46.136 +0000
        categories: [부스트코스-PreCourse]
        tags: ['부스트코스', '프리코스']
        description: 변수의 움직임에 따른 함수값의 변화(변화율 또는 기울기)를 측정하기 위한 도구f(x)와 f(x+h)의 변화량을 계산하는데, h를 극한으로 보내면 f(x)에서의 접선의 기울기(변화율)를 알 수 있다.딥러닝의 최적화에서 제일 많이 사용하는 기법이다.손으로 직접 계산하는 대
        
        
        ---

        ## 미분(differentiation)

- 변수의 움직임에 따른 함수값의 변화(변화율 또는 기울기)를 측정하기 위한 도구
- f(x)와 f(x+h)의 변화량을 계산하는데, h를 극한으로 보내면 f(x)에서의 접선의 기울기(변화율)를 알 수 있다.
- 딥러닝의 최적화에서 제일 많이 사용하는 기법이다.

- 손으로 직접 계산하는 대신 컴퓨터가 계산해 줄 수 있다.

```python
import sympy as sym # 미분 함수를 제공하는 모듈
from sympy.abc import x

sym.diff(sym.poly(3x**2 + x + 3), x) # diff함수의 첫번째 인자로 식을 넣고, 두번쨰 인자로 미분할 변수를 넣는다.
# Poly(6*x + 1, x, domain='ZZ')
```

## 미분의 응용

- 미분을 통해 접선의 기울기를 알면, 어느방향으로 점을 움직여야 함수값이 증가 또는 감소하는지 알 수 있다.
	- 미분값이 음수일 경우 x가 커지면 함수값은 감소, 양수일 경우 x가 커지면 함수값이 증가

- **미분값의 부호에 관계없이 함수값을 증가시키고 싶다면 미분값을 더하고, 감소시키고 싶다면 미분값을 빼면 된다.**

> `f'(x) < 0`일 경우, `x+f'(x)` < `x` 이므로 왼쪽으로 이동하며, 함수값이 증가한다..
> `f'(x) > 0`일 경우 `x+f'(x)` > `x` 이므로 `f(x+f'(x)`는 오른쪽으롷 이동하며 함수값이 증가한다.

- 이를 응용한 것이 경사상승법, 경사하강법이다.

## 경사상승법(gradient ascent)

- 미분값을 계속 더하면 함수값은 계속 증가하다 함수의 **극대값**에서 멈추게 된다. (미분값이 0이 되므로)
- 이렇게 극대값을 구하는 방법을 경사상승법이라 하며, 목적함수를 최대화 할 때 사용한다.

## 경사하강법(gradient descent)

- 미분값을 계속 빼면 함수값은 계속 감소하다 함수의 **극소값**에서 멈추게 된다.
- 이렇게 극소값을 구하는 방법을 경사하강법이라 하며, 목적함수를 최소화 할 때 사용한다.
- 학습은 목적함수를 최소하는 것이 목표이므로 가장 많이 사용하는 방법이다.

### 알고리즘 구현

> input : gradient 함수(미분을 계산하는 함수), init(초기값), lr(학습률), eps(알고리즘 종료조건)
> - lr(학습률) : 학습률이 클 수록 업데이트하는 속도가 빨라진다. 그러나 수렴에 쉽게 도달할 수 없게 되므로 주의해서 제어해야 한다.
> - eps(종료 조건) : 컴퓨터 계산에서는 정확히 0에 도달하는 것이 불가능하므로, 미분값의 절대값이 eps보다 작을 때 함수를 종료하게 된다.

```python
# pseudo code
var = init
grad = gradient(var) # 미분값을 구한다.
while abs(grad) > eps: # 미분값의 절대값이 eps보다 작거나 같으면 알고리즘 종료
	var = var - lr * grad # 학습률에 따라 미분값을 빼 함수값을 감소시킨다.
    grad = gradient(var) # 새로운 미분값을 구한다.
```

## 벡터의 경사하강법

- 미분을 이용한 경사하강법은 변수가 1개일 때 사용할 수있는 방법이다.
- 하지만 벡터는 여러가지 변수가 존재하므로 **편미분(partial differentiation)**을 사용해야한다.

- 편미분 또한 `sympy`의 `diff()` 메소드를 통해 구현이 가능하다.

```python
import sympy as sym
from sympy.abc import x, y

sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x) # x로 편미분

# Poly(2*x + 2*y - sin(x + 2*y)
```

### 그레디언트(gradient) 벡터

- 벡터가 입력인 다변수 함수를 편미분하여 얻은 새로운 벡터를 말한다.
- 벡터의 각각의 성분에 대해 편미분 한것으로, d차원의 벡터라면 d개만큼 편미분을 계산할 수 있다.
- 경사하강법에 사용할 수 있다.

> `∇f` : `nabla`라고 하는 기호를 이용하여 편미분된 벡터를 표시한다. (미분의 `f'(x)`와 동일 역할)

### 그레디언트 벡터의 경사하강법

- `-∇f`의 모든 성분은 극소점으로 향하는 벡터가 된다.
- `f(x, y)`에서 `-∇f`는 극소점을 향해 가장 빠르게 감소하는 방향과 같다.
- 즉 그레디언트 벡터를 통해 극대점 또는 극소점으로 향하는 방향을 알 수 있다.

### 그레디언트 벡터 알고리즘

- 기본적으로 미분 알고리즘과 동일하지만, 절대값 대신 `norm`을 계산해서 종료조건을 설정한다.

```python
# pseudo code
var = init
grad = gradient(var) # 미분값을 구한다.
while norm(grad) > eps: # 미분값의 노름이 eps보다 작거나 같으면 알고리즘 종료
	var = var - lr * grad # 학습률에 따라 미분값을 빼 함수값을 감소시킨다.
    grad = gradient(var) # 새로운 미분값을 구한다.
```

## 결론

벡터의 편미분을 통해 벡터를 input으로 받은 함수의 극소·극대 값을 구할 수 있다.
이를 활용하여 최적화가 가능하다. 

이번 부스트코스 프리코스에는 매운맛 강의가 빠져있다.
좀 더 준비가 되면 배울 기회가 생기겠지...


        